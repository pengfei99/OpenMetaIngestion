{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Openmetadata spark agent\n",
    "\n",
    "There is a spark agent which can capture the information from the spark operation. It can generate tables(spark write dataframe to database table) and the lineage between tables.\n",
    "\n",
    "https://github.com/open-metadata/openmetadata-spark-agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "700fc3646829524c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from creds import dbUser, dbPasswd, dbHost, dbPort, om_admin_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:05:36.001584823Z",
     "start_time": "2024-07-01T11:05:35.820763790Z"
    }
   },
   "id": "a0306bbd9efe4aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 13:06:06 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "24/07/01 13:06:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/07/01 13:06:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "jar_path=\"/home/pengfei/git/OpenMetaIngestion/jars\"\n",
    "\n",
    "enable_meta=False\n",
    "\n",
    "if enable_meta:\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local\")\n",
    "        .appName(\"localTestApp\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            f\"{jar_path}/openmetadata-spark-agent-1.0.jar,{jar_path}/mysql-connector-java-8.0.30.jar\")\n",
    "        .config(\n",
    "            \"spark.extraListeners\",\"org.openmetadata.spark.agent.OpenMetadataSparkListener\")\n",
    "        .config(\"spark.openmetadata.transport.hostPort\", \"http://datacatalog.casd.local\")\n",
    "        .config(\"spark.openmetadata.transport.type\", \"openmetadata\")\n",
    "        .config(\"spark.openmetadata.transport.jwtToken\", om_admin_token)\n",
    "        .config(\"spark.openmetadata.transport.pipelineServiceName\", \"test_pipeline_service1\")\n",
    "        .config(\"spark.openmetadata.transport.pipelineName\", \"my_pipeline_name\")\n",
    "        .config(\"spark.openmetadata.transport.pipelineSourceUrl\", \"http://casd.local/path/to/pipeline\")\n",
    "        .config(\"spark.openmetadata.transport.pipelineDescription\", \"My_ETL_Pipeline\")\n",
    "        .config(\"spark.openmetadata.transport.databaseServiceNames\", \"constance\")\n",
    "        .config(\"spark.openmetadata.transport.timeout\", \"30\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "else:\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            f\"{jar_path}/mysql-connector-java-8.0.30.jar\")\n",
    "        .appName(\"localTestApp\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:09.643396166Z",
     "start_time": "2024-07-01T11:06:02.731898441Z"
    }
   },
   "id": "3dc9ca497f6fac92"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+\n",
      "|UID|Name|FLX_DIS_DTD|\n",
      "+---+----+-----------+\n",
      "+---+----+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read from MySQL Table\n",
    "db_name=\"constance1\"\n",
    "table_name=\"ct_E2021_F2010_DCIR_BIO\"\n",
    "employee_df = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", f\"jdbc:mysql://{dbHost}:{dbPort}/{db_name}\")\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n",
    "    .option(\"dbtable\", table_name)\n",
    "    .option(\"user\", f\"{dbUser}\")\n",
    "    .option(\"password\", f\"{dbPasswd}\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "employee_df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:20.652693917Z",
     "start_time": "2024-07-01T11:06:14.403208744Z"
    }
   },
   "id": "66957759bf4ad273"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- FLX_DIS_DTD: date (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "employee_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:33.290624291Z",
     "start_time": "2024-07-01T11:06:33.243318338Z"
    }
   },
   "id": "caa225f68f75c5cf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "# creat a new table with the dataframe employee_df\n",
    "target_db_name = \"pengfei_test\"\n",
    "\n",
    "employee_df.write.format(\"jdbc\") \\\n",
    ".option(\"url\", f\"jdbc:mysql://{dbHost}:{dbPort}/{db_name}\") \\\n",
    ".option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    ".option(\"dbtable\", target_db_name) \\\n",
    ".option(\"user\", f\"{dbUser}\") \\\n",
    ".option(\"password\", f\"{dbPasswd}\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:35.925421136Z",
     "start_time": "2024-07-01T11:07:35.538405689Z"
    }
   },
   "id": "aa3e4db9ab874d71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create table by using view and sql\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "813b2e058a2f023a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data_user = [\n",
    "    (1, \"Alice\", 29),\n",
    "    (2, \"Bob\", 31),\n",
    "    (3, \"Cathy\", 23)\n",
    "]\n",
    "schema_user = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df_user=  spark.createDataFrame(data_user,schema_user)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:41.498683890Z",
     "start_time": "2024-07-01T11:06:41.239246393Z"
    }
   },
   "id": "c52cb0ffca43839d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 23|\n",
      "+---+-----+---+\n"
     ]
    }
   ],
   "source": [
    "df_user.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:48.515851860Z",
     "start_time": "2024-07-01T11:06:44.348706352Z"
    }
   },
   "id": "ea1ffe037c11bd47"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data_order = [\n",
    "    (1, 23, 1),\n",
    "    (2, 24, 2),\n",
    "    (3, 25, 3)\n",
    "]\n",
    "schema_order = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"uid\", IntegerType(), True)\n",
    "])\n",
    "df_order =  spark.createDataFrame(data_order, schema_order)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:06:56.166582324Z",
     "start_time": "2024-07-01T11:06:56.064902871Z"
    }
   },
   "id": "7bbec496cca715fb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+\n",
      "| id|product_id|uid|\n",
      "+---+----------+---+\n",
      "|  1|        23|  1|\n",
      "|  2|        24|  2|\n",
      "|  3|        25|  3|\n",
      "+---+----------+---+\n"
     ]
    }
   ],
   "source": [
    "df_order.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:08.808056028Z",
     "start_time": "2024-07-01T11:07:08.359125373Z"
    }
   },
   "id": "1bff680dff6db8a3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df_user.createOrReplaceTempView(\"test_user\")\n",
    "df_order.createOrReplaceTempView(\"test_order\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:12.777592464Z",
     "start_time": "2024-07-01T11:07:12.695972016Z"
    }
   },
   "id": "bd24805230e0ac14"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 23|\n",
      "+---+-----+---+\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.sql(\"\"\"SELECT id, name, age FROM test_user ;\"\"\")\n",
    "test_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:14.637172463Z",
     "start_time": "2024-07-01T11:07:13.855268073Z"
    }
   },
   "id": "735af3dac16a91bd"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "user_order_df=spark.sql(\"\"\"SELECT test_user.id as uid, \n",
    "             test_order.id as oid, \n",
    "             test_order.product_id,\n",
    "             test_user.age\n",
    "             FROM test_user \n",
    "             INNER JOIN test_order \n",
    "             ON test_user.id=test_order.uid;\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:20.377026006Z",
     "start_time": "2024-07-01T11:07:20.306684419Z"
    }
   },
   "id": "9087a1b5c213e073"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+---+\n",
      "|uid|oid|product_id|age|\n",
      "+---+---+----------+---+\n",
      "|  1|  1|        23| 29|\n",
      "|  2|  2|        24| 31|\n",
      "|  3|  3|        25| 23|\n",
      "+---+---+----------+---+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_order_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T11:07:24.536755627Z",
     "start_time": "2024-07-01T11:07:21.667668313Z"
    }
   },
   "id": "d79da70b110c7d63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fe47a6b31f136b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
