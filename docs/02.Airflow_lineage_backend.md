# Airflow linage backend

The airflow linage OM backend allows airflow to ingest pipeline into configured OM server.

## 1. Installation

If you run your airflow server inside a python virtual env, you need to install the following package into the virtual
env.

```shell
# make sure the ingestion package version is the same as the OM server version
pip install "openmetadata-ingestion==x.y.z"
```

> If you use the ingestion docker image, this package is already installed

## 2. Adding lineage backend config into airflow server

Go to the root path where you installed the airflow. You should find a config file called **airflow.cfg**.
In this file, you will find a section called `lineage`. Add the below config into the lineage section:

```text
[lineage]
backend = airflow_provider_openmetadata.lineage.backend.OpenMetadataLineageBackend
airflow_service_name = <om-pipeline-service-name>
openmetadata_api_endpoint = http://<om-server-url>/api
auth_provider_type = openmetadata
jwt_token = <your-token>
```

You need to adapt the above config for your environment. For example, the below example is a config for the docker compose
deployment.

```text
[lineage]
backend = airflow_provider_openmetadata.lineage.backend.OpenMetadataLineageBackend
airflow_service_name = test-pipeline-service
openmetadata_api_endpoint = http://openmetadata_server/api
auth_provider_type = openmetadata
jwt_token = <your-token>
```

> In docker compose deployment, the url for the openmetadata server container is the service name in the docker compose 
> yaml file. The service name must exist in your OM server

### Adapt your base url

You will notice that the generated pipeline in the OM server uses the value of **base_url** in the **airflow.cfg**.

To make sure the generated url can link to your airflow server, you need to change the base url to the correct url

```text
# change the default value to your server url
base_url = http://localhost:8080/
```

## 3. Add a test dag and test the backend ingestion

Now you can access the airflow container and add a new DAG


```shell
# get a shell of your airflow container
sudo docker exec -it -u root openmetadata_ingestion /bin/bash

# install a text editor
apt update
apt install vim

# go to the root dag dir
cd /opt/airflow/dags
vim sample_dag1.py

# add the below dag code, to activate the OM backend in airflow to ingest the pipeline, you only
# need to run the dag 1 time. If you don't want to run it, you can use the test mode
airflow dags test <dag_id>    
```

```python
from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.python import PythonOperator

# These args will get passed on to each operator
# You can override them on a per-task basis during operator initialization
from airflow_provider_openmetadata.lineage.callback import success_callback, failure_callback

default_args = {
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'on_failure_callback': failure_callback,
    'on_success_callback': success_callback,
}

with DAG(
        dag_id='user_order_lineage_python',
        default_args=default_args,
        description='A simple tutorial DAG',
        schedule_interval=timedelta(days=1),
        start_date=datetime(2021, 1, 1),
        catchup=False,
        tags=[],
) as dag:
    def show_sql(query: str) -> None:
        print(query)


    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = PythonOperator(
        task_id='task_1',
        python_callable=show_sql,
        op_kwargs={'query': 'select * from table'},
        inlets={
            "group_A": ["test-db-service.test-db.test-schema.test_user",
                        "test-db-service.test-db.test-schema.test_order"]
        }

    )

    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG
    dag.doc_md = """
    This is a documentation placed anywhere
    """  # otherwise, type it like this

    t2 = PythonOperator(
        task_id='task_2',
        depends_on_past=False,
        python_callable=show_sql,
        op_kwargs={'query': 'select * from table'},
        outlets={
            "group_A": ["test-db-service.test-db.test-schema.user_order_cube"]
        }

    )

    t1 >> t2
```

> After running the dag one time, a new pipeline will be crated in the given pipeline service(configured in airflow.cfg)
> You should be able to view the new generated pipeline in OM server
> 
If after running the dag, you see nothing inside your OM server, you need to debug the airflow OM BACKEND
config in **airflow.cfg**.